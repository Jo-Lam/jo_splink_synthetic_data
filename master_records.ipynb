{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd \n",
    "pd.options.display.max_columns = 1000\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person = spark.read.parquet(\"scrape_wikidata/processed_data/step_1_one_line_per_person/page000_0_to_2000.parquet\")\n",
    "df_person.createOrReplaceTempView(\"df_person\")\n",
    "df_person.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_names = spark.read.parquet(\"scrape_wikidata/raw_data/names/\")\n",
    "df_names.createOrReplaceTempView(\"df_names\")\n",
    "sql = \"\"\"\n",
    "\n",
    "with concat as (\n",
    "select original_name, alt_name\n",
    "from df_names\n",
    "union all\n",
    "select alt_name as original_name, original_name as alt_name\n",
    "from df_names)\n",
    "\n",
    "select distinct original_name, alt_name\n",
    "from concat \n",
    "\n",
    "\"\"\"\n",
    "df_names = spark.sql(sql)\n",
    "df_names.createOrReplaceTempView(\"df_names\")\n",
    "\n",
    "\n",
    "df_freq_fn = spark.read.parquet(\"scrape_wikidata/processed_data/step_x_births_namrefreq/name_counts_from_births_register.parquet\")\n",
    "df_freq_fn = df_freq_fn.withColumnRenamed(\"forename\", \"name\")\n",
    "df_freq_sn = spark.read.parquet(\"scrape_wikidata/processed_data/step_x_births_namrefreq/surname_counts_from_births_register.parquet\")\n",
    "df_freq_sn = df_freq_fn.withColumnRenamed(\"surname\", \"name\")\n",
    "df_freq = df_freq_fn.union(df_freq_sn)\n",
    "df_freq.createOrReplaceTempView(\"df_freq\")\n",
    "sql = \"\"\"\n",
    "select name, sum(count) as count\n",
    "from df_freq\n",
    "group by name\n",
    "order by count desc\n",
    "\"\"\"\n",
    "df_freq = spark.sql(sql)\n",
    "df_freq.createOrReplaceTempView(\"df_freq\")\n",
    "\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "select df_names.*, df_freq.count\n",
    "from df_names\n",
    "left join df_freq\n",
    "on lower(df_names.alt_name) = lower(df_freq.name)\n",
    "where df_freq.count > 5\n",
    "\"\"\"\n",
    "df_names = spark.sql(sql)\n",
    "\n",
    "df_names.createOrReplaceTempView(\"df_names\")\n",
    "\n",
    "df_names = spark.sql(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "select original_name, collect_list(struct(alt_name, count)) as alt_names\n",
    "from df_names\n",
    "where alt_name rlike '^[A-Za-z]+$'\n",
    "    \n",
    "group by original_name\n",
    "\"\"\")\n",
    "df_names.createOrReplaceTempView(\"df_names\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person postcode lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_point_postcode = spark.read.parquet(\"scrape_wikidata/processed_data/step_2_person_postcode_lookups/page000_0_to_2000.parquet\")\n",
    "df_point_postcode.createOrReplaceTempView(\"df_point_postcode\")\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "select person, collect_list(nearby_postcodes) as nearby_postcodes\n",
    "from df_point_postcode\n",
    "group by person\n",
    "\n",
    "\"\"\"\n",
    "df_point_postcode = spark.sql(sql)\n",
    "df_point_postcode.createOrReplaceTempView(\"df_point_postcode\")\n",
    "df_point_postcode.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from df_person where human = 'Q38082'\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of given names and family names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person = spark.read.parquet(\"scrape_wikidata/processed_data/step_1_one_line_per_person/page000_0_to_2000.parquet\")\n",
    "df_person.createOrReplaceTempView(\"df_person\")\n",
    "df_person.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "data_list = [\n",
    "    {\"to_filter_out\": ['Rt.', 'Hon.', 'Sir', 'Rev.', 'Lady', 'Duke', 'of', 'Dr.', 'Dr', 'Baron', 'The', 'and', 'last', 'Baronet']},\n",
    "\n",
    "        ]\n",
    "\n",
    "df_filter = spark.createDataFrame(Row(**x) for x in data_list)\n",
    "df_filter.createOrReplaceTempView(\"df_filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given names \n",
    "\n",
    "split_given_name = \"ifnull(split(given_nameLabel,' \\\\\\\\| '), array())\"\n",
    "\n",
    "\n",
    "hl_rr = \"regexp_replace(humanLabel, ',(.+)', '')\"\n",
    "label_parts_expr = f\"split({hl_rr}, ' \\\\\\\\| ')\"\n",
    "split_label = f\"ifnull(flatten(transform({label_parts_expr}, x -> slice(split(x, ' '), 1, size(split(x, ' ')) - 1))), array())\"\n",
    "\n",
    "rr_x = \"regexp_replace(x, ',(.+)', '')\"\n",
    "alt_label_parts_expr = \"split(humanAltlabel, ', ')\"\n",
    "split_alt_label = f\"ifnull(flatten(transform({alt_label_parts_expr}, x -> slice(split({rr_x}, ' '), 1, size(split({rr_x}, ' ')) - 1))), array())\"\n",
    "\n",
    "\n",
    "union_all_names = f\"array_union({split_given_name}, array_union({split_label},{split_alt_label}))\"\n",
    "filter_union_all_names = f\"array_except({union_all_names}, to_filter_out)\"\n",
    "\n",
    "filter_union_all_given_names =f\"filter({filter_union_all_names}, x -> x not rlike '[0-9]')\"\n",
    "\n",
    "# Family names\n",
    "split_family_name = \"ifnull(split(family_nameLabel,' \\\\\\\\| '), array())\"\n",
    "\n",
    "hl_rr = \"regexp_replace(humanLabel, ',(.+)', '')\"\n",
    "label_parts_expr = f\"split({hl_rr}, ' \\\\\\\\| ')\"\n",
    "split_label = f\"ifnull(flatten(transform({label_parts_expr}, x -> slice(split(x, ' '), -1,1))), array())\"\n",
    "\n",
    "rr_x = \"regexp_replace(x, ',(.+)', '')\"\n",
    "alt_label_parts_expr = \"split(humanAltlabel, ', ')\"\n",
    "split_alt_label = f\"ifnull(flatten(transform({alt_label_parts_expr}, x -> slice(split({rr_x}, ' '), -1, 1))), array())\"\n",
    "\n",
    "\n",
    "union_all_names = f\"array_union({split_family_name}, array_union({split_label},{split_alt_label}))\"\n",
    "filter_union_all_family_names = f\"array_except({union_all_names}, to_filter_out)\"\n",
    "\n",
    "\n",
    "\n",
    "sql = f\"\"\"\n",
    "select\n",
    "humanLabel, humanAltLabel, given_nameLabel, family_nameLabel,\n",
    "\n",
    "\n",
    "\n",
    "{filter_union_all_given_names} as given_names_array,\n",
    "{filter_union_all_family_names} as family_names_array\n",
    "\n",
    "from df_person\n",
    "cross join df_filter\n",
    "\n",
    "\n",
    "limit 1000\n",
    "\"\"\"\n",
    "spark.sql(sql).toPandas().sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that given names are NOT in order - we do not scrape the ordinality of given names so order cannot easily be imposed\n",
    "\n",
    "concat(split(given_namelabel), split(human_label)[:]\n",
    "\n",
    "def split_field(col_name, num_cols=3):\n",
    "    parts = [f\"split({col_name}, ' \\\\\\\\| ')[{i-1}] as {col_name}_{i}\" for i in range(1,num_cols+1)]\n",
    "    return \", \".join(parts)\n",
    "\n",
    "sql = f\"\"\"\n",
    "select \n",
    "humanlabel, humanaltlabel, given_namelabel, family_namelabel, birth_name, \n",
    "{split_field('given_namelabel')},\n",
    " {split_field('family_namelabel',2)}\n",
    "from df_person\n",
    "\n",
    "\"\"\"\n",
    "df_person_split_names = spark.sql(sql)\n",
    "df_person_split_names.createOrReplaceTempView(\"df_person_split_names\")\n",
    "df_person_split_names.limit(1000).toPandas().sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "select \n",
    "h.human, \n",
    "\n",
    "h.humanlabel,\n",
    "split(h.humanaltlabel, \", \") as humanaltlabel,\n",
    "substr(h.dob,1,10) as dob, \n",
    "\n",
    "\n",
    "split(h.country_citizenlabel, ' \\\\\\\\| ') as country_citizenship,\n",
    "place_birthlabel as birth_place,\n",
    "birth_countrylabel as birth_country,\n",
    "sex_or_genderlabel as gender,\n",
    "\n",
    "\n",
    "\n",
    "residencelabel as residence_place,\n",
    "residence_countrylabel as residence_country,\n",
    "\n",
    "pc.nearby_postcodes[0][0].postcode as fake_postcode,\n",
    "pc.nearby_postcodes[0][0].lat as fake_lat,\n",
    "pc.nearby_postcodes[0][0].lng as fake_lng,\n",
    "\n",
    "pc.nearby_postcodes,\n",
    "h.given_namelabel_1 as given_name_1,\n",
    "n1.alt_names as alt_given_name_1,\n",
    "h.given_namelabel_2 as given_name_2,\n",
    "n2.alt_names as alt_given_name_2,\n",
    "h.given_namelabel_3 as given_name_3,\n",
    "n3.alt_names as alt_given_name_3,\n",
    "h.family_namelabel_1 as family_name_1,\n",
    "n4.alt_names as alt_family_name_1,\n",
    "h.family_namelabel_2 as family_name_2,\n",
    "n5.alt_names as alt_family_name_2\n",
    "\n",
    "from df_person_split_names as h\n",
    "\n",
    "left join df_names as n1\n",
    "on h.given_namelabel_1 = n1.original_name\n",
    "\n",
    "left join df_names as n2\n",
    "on h.given_namelabel_2 = n2.original_name\n",
    "\n",
    "left join df_names as n3\n",
    "on h.given_namelabel_3 = n3.original_name\n",
    "\n",
    "left join df_names as n4\n",
    "on h.family_namelabel_1 = n4.original_name\n",
    "\n",
    "left join df_names as n5\n",
    "on h.family_namelabel_2 = n5.original_name\n",
    "\n",
    "left join df_point_postcode as pc\n",
    "on h.human = pc.person\n",
    "\n",
    "\"\"\"\n",
    "df_final = spark.sql(sql)\n",
    "df_final.createOrReplaceTempView(\"df_final\")  \n",
    "df_final = df_final.repartition(1)\n",
    "df_final.write.mode('overwrite').parquet(\"scrape_wikidata/clean_data/master_data/\")\n",
    "# df_final.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
