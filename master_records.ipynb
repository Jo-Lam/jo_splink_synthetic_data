{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 20\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_person = spark.read.parquet(\"scrape_wikidata/processed_data/step_1_one_line_per_person/page000_0_to_2000.parquet\")\n",
    "df_person = spark.read.parquet(\"scrape_wikidata/processed_data/step_1_one_line_per_person/\")\n",
    "\n",
    "for col in [\"humanlabel\", \"humanaltlabel\", \"birth_name\", \"birth_name\", \"given_namelabel\", \"family_namelabel\"]:\n",
    "    df_person = df_person.withColumn(col, expr(f'lower({col})'))\n",
    "\n",
    "df_person.createOrReplaceTempView(\"df_person\")\n",
    "# spark.sql(\"select * from df_person where human = 'Q38082'\").toPandas() lewis caroll\n",
    "df_person.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_data.names import get_df_given_names_with_freqs, get_df_family_names_with_freqs\n",
    "df_given_names = get_df_given_names_with_freqs(spark)\n",
    "df_family_names = get_df_family_names_with_freqs(spark)\n",
    "df_given_names.createOrReplaceTempView(\"df_given_names\")\n",
    "df_family_names.createOrReplaceTempView(\"df_family_names\")\n",
    "display(df_given_names.limit(2).toPandas())\n",
    "display(df_family_names.limit(2).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person postcode lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_data.utils import get_person_nearby_postcodes_lookup\n",
    "df_point_postcode = get_person_nearby_postcodes_lookup(spark)\n",
    "df_point_postcode.createOrReplaceTempView(\"df_point_postcode\")\n",
    "\n",
    "\n",
    "df_point_postcode.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = spark.read.parquet(\"scrape_wikidata/raw_data/occupations/\")\n",
    "occupations.createOrReplaceTempView(\"occupations\")\n",
    "occupations.limit(2).toPandas()\n",
    "\n",
    "sql = \"\"\"\n",
    "select human, array_distinct(collect_list(occupationLabel)) as occupation_options\n",
    "from occupations\n",
    "group by human\n",
    "\"\"\"\n",
    "\n",
    "occupations = spark.sql(sql)\n",
    "occupations.createOrReplaceTempView(\"occupations\")\n",
    "occupations.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of given names and family names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_data.names import get_df_filter, name_split\n",
    "df_person = name_split(df_person, spark)\n",
    "df_person.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dupe_country_citizen = \"\"\"\n",
    "array_distinct(\n",
    "    transform(\n",
    "        split(h.country_citizenlabel, ' \\\\\\\\| '), \n",
    "            x -> case \n",
    "                    when x = 'United Kingdom of Great Britain and Ireland' then 'United Kingdom' \n",
    "                    else x \n",
    "                    end\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sql = f\"\"\"\n",
    "select \n",
    "h.row_num,\n",
    "h.human, \n",
    "h.humanlabel,\n",
    "split(h.humanaltlabel, \", \") as humanaltlabel,\n",
    "occ.occupation_options,\n",
    "substr(h.dob,1,10) as dob, \n",
    "\n",
    "\n",
    "{remove_dupe_country_citizen}  as country_citizenship,\n",
    "\n",
    "\n",
    "place_birthlabel as birth_place,\n",
    "birth_countrylabel as birth_country,\n",
    "sex_or_genderlabel as gender,\n",
    "\n",
    "\n",
    "\n",
    "residencelabel as residence_place,\n",
    "residence_countrylabel as residence_country,\n",
    "\n",
    "pc.birthplace_nearby_locs,\n",
    "pc.residence_nearby_locs,\n",
    "pc.random_nearby_locs,\n",
    "\n",
    "h.given_name_1 as given_name_1,\n",
    "n1.alt_names as alt_given_name_1,\n",
    "h.given_name_2 as given_name_2,\n",
    "n2.alt_names as alt_given_name_2,\n",
    "h.given_name_3 as given_name_3,\n",
    "n3.alt_names as alt_given_name_3,\n",
    "h.family_name_1 as family_name_1,\n",
    "n4.alt_names as alt_family_name_1,\n",
    "h.family_name_2 as family_name_2,\n",
    "n5.alt_names as alt_family_name_2\n",
    "\n",
    "from df_person as h\n",
    "\n",
    "left join df_given_names as n1\n",
    "on lower(h.given_name_1) = n1.original_name\n",
    "\n",
    "left join df_given_names as n2\n",
    "on lower(h.given_name_2) = n2.original_name\n",
    "\n",
    "left join df_given_names as n3\n",
    "on lower(h.given_name_3) = n3.original_name\n",
    "\n",
    "left join df_family_names as n4\n",
    "on lower(h.family_name_1) = n4.original_name\n",
    "\n",
    "left join df_family_names as n5\n",
    "on lower(h.family_name_2) = n5.original_name\n",
    "\n",
    "left join df_point_postcode as pc\n",
    "on h.human = pc.person\n",
    "\n",
    "left join occupations  as occ\n",
    "on h.human = occ.human\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "df_final = spark.sql(sql)\n",
    "df_final.createOrReplaceTempView(\"df_final\")  \n",
    "\n",
    "\n",
    "df_final.limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now want to link parent back onto this table, and their location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_family = spark.read.parquet(\"scrape_wikidata/raw_data/family/parent_child/\")\n",
    " \n",
    "df_one_parent = df_family.dropDuplicates([\"child\"])\n",
    "# df_one_child = df_family.dropDuplicates([\"human\"])\n",
    "df_one_parent.createOrReplaceTempView(\"df_one_parent\") \n",
    "# df_one_child.createOrReplaceTempView(\"df_one_child\") \n",
    "df_family.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "\n",
    "select \n",
    "    df.*, \n",
    "    p.human as parent, \n",
    "    p_details.birthplace_nearby_locs as parent_birthplace_loc, \n",
    "    p_details.random_nearby_locs as parent_random_loc\n",
    "\n",
    "\n",
    "from df_final as df\n",
    "left join df_one_parent as p\n",
    "on df.human = p.child\n",
    "left join df_final as p_details\n",
    "on p.human = p_details.human\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# No need for child as if parents take childs details and children take parents, then you don't achieve parents and children having the same address!\n",
    "# left join df_one_child as c\n",
    "# on df.human = c.human\n",
    "# left join df_final as c_details\n",
    "# on c.child = c_details.human\n",
    "df_final_with_parents = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_with_parents = df_final_with_parents.repartition(1)\n",
    "df_final_with_parents.write.mode('overwrite').parquet(\"scrape_wikidata/clean_data/master_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_parquet(\"scrape_wikidata/clean_data/master_data/\")\n",
    "len(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.query(\"human == 'Q42'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.query(\"human == 'Q42'\").iloc[0,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [c for c in d1.columns if 'name' not in c]\n",
    "d1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
